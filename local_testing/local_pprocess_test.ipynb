{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully and saved as 'config_file.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://loaded_config_3.txt [Content-Type=text/plain]...\n",
      "- [1 files][    4.0 B/    4.0 B]                                                \n",
      "Operation completed over 1 objects/4.0 B.                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to Google Cloud Storage successfully.\n",
      "File downloaded successfully and saved as 'upland_gcps_200m.kml'.\n",
      "File downloaded successfully and saved as 'flightplan.kml'.\n",
      "File downloaded successfully and saved as 'manifest.csv'.\n",
      "File downloaded successfully and saved as 'gcp_list_init.txt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://downloaded_supporting_dat_3.txt [Content-Type=text/plain]...\n",
      "/ [1 files][    4.0 B/    4.0 B]                                                \n",
      "Operation completed over 1 objects/4.0 B.                                        \n",
      "/opt/miniconda3/envs/ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3338: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to Google Cloud Storage successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://partitioned_area_3.txt [Content-Type=text/plain]...\n",
      "/ [1 files][    4.0 B/    4.0 B]                                                \n",
      "Operation completed over 1 objects/4.0 B.                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to Google Cloud Storage successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3338: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# subprocess.run(['pip3', 'install', '--upgrade', 'pip'], check=True)\n",
    "# subprocess.run(['sudo', 'apt-get', 'install', 'gdal-bin', 'libgdal-dev', 'libspatialindex-dev'], check=True, input=b'y\\n')\n",
    "\n",
    "# packages = ['cython','pyproj','geopandas','pandas','numpy','shapely','fiona','rasterio','scipy', 'rtree','pygeos']\n",
    "\n",
    "# for p in packages:\n",
    "#   subprocess.check_call(['pip3', 'install', p])\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon, box, mapping\n",
    "from pyproj import CRS\n",
    "import fiona\n",
    "import requests\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import Resampling\n",
    "from scipy.spatial import Voronoi\n",
    "\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully and saved as '{save_path}'.\")\n",
    "    else:\n",
    "        print(f\"Error occurred while downloading file from '{url}'.\")\n",
    "        \n",
    "def get_metadata(attribute):\n",
    "    curl_command = [\n",
    "        \"curl\",\n",
    "        \"-H\",\n",
    "        \"Metadata-Flavor: Google\",\n",
    "        f\"http://metadata/computeMetadata/v1/instance/attributes/{attribute}\"\n",
    "    ]\n",
    "    result = subprocess.run(curl_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "    output = result.stdout.strip()\n",
    "    return output\n",
    "\n",
    "def expand_to_gcps(focal_poly, gcps, gcp_cutoff=5, step_sz=30, base_buffer=50):\n",
    "    focal_poly = focal_poly.buffer(base_buffer)\n",
    "    count = sum(gcps.within(focal_poly.geometry.iloc[0]))\n",
    "    \n",
    "    if count < gcp_cutoff:\n",
    "        while count < gcp_cutoff:\n",
    "            focal_poly = focal_poly.buffer(step_sz)\n",
    "            count = sum(gcps.within(focal_poly.geometry.iloc[0]))\n",
    "    \n",
    "    return focal_poly\n",
    "\n",
    "def load_kml(path):\n",
    "  df = gpd.GeoDataFrame()\n",
    "\n",
    "  # iterate over layers\n",
    "  for layer in fiona.listlayers(path):\n",
    "      s = gpd.read_file(path, driver='KML', layer=layer)\n",
    "      df = pd.concat([df, s], ignore_index=True)\n",
    "  return df\n",
    "\n",
    "def copy_to_gcs(local_file_path, bucket_name):\n",
    "    command = ['gsutil', 'cp', '-r', local_file_path, 'gs://{}/'.format(bucket_name)]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print('File copied to Google Cloud Storage successfully.')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('Error occurred while copying the file to Google Cloud Storage:')\n",
    "        print(e)\n",
    "\n",
    "def mask_to_gdf(gdf, raster_path, output_path):\n",
    "    # Read the raster file\n",
    "    src = rasterio.open(raster_path)\n",
    "\n",
    "    # Reproject the GeoDataFrame to match the projection of the raster, if needed\n",
    "    gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "    # Mask the raster using the GeoDataFrame's geometry\n",
    "    out_image, out_transform = mask(src, gdf.geometry, crop=True)\n",
    "\n",
    "    # Update the metadata of the cropped raster\n",
    "    out_meta = src.meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"dtype\": out_image.dtype,\n",
    "        \"compress\": src.compression.value if src.compression else \"none\"\n",
    "    })\n",
    "\n",
    "    # Save the cropped raster to a new file\n",
    "    with rasterio.open(output_path, 'w', **out_meta) as dst:\n",
    "        # Use the original raster's block size and resampling method for better compression\n",
    "        dst.write(out_image)\n",
    "\n",
    "def process_images(batch, output_bucket, ortho_res, cutline, suffix, gcp_list_path):\n",
    "   # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "    # Create an 'images' subdirectory\n",
    "    images_dir = os.path.join(temp_dir, 'images')\n",
    "    os.mkdir(images_dir)\n",
    "\n",
    "    if gcp_list_path is not None:\n",
    "        shutil.move(gcp_list_path, temp_dir)\n",
    "\n",
    "    for url in batch:\n",
    "        try:\n",
    "            subprocess.run(['wget', '-q','-P', images_dir, url], check=True)\n",
    "            print(f\"File downloaded successfully from {url}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error occurred while downloading file from {url}:\")\n",
    "            print(e)\n",
    "    \n",
    "    docker_command = [\n",
    "        \"docker\", \"run\", \"--rm\",\n",
    "        \"-v\", \"{}:/datasets/code\".format(temp_dir),\n",
    "        \"opendronemap/odm\", \"--project-path\", \"/datasets\",\n",
    "        \"--orthophoto-resolution\", f\"{ortho_res}\",\n",
    "        \"--fast-orthophoto\",\n",
    "        \"--force-gps\",\n",
    "        \"--min-num-features\", \"15000\"\n",
    "    ]\n",
    "\n",
    "    process = subprocess.call(docker_command)#, check=True)\n",
    "    \n",
    "    ortho = os.path.join(temp_dir,'odm_orthophoto/odm_orthophoto.tif')\n",
    "    report = os.path.join(temp_dir,'odm_report/report.pdf')\n",
    "    ortho_new = ortho.replace('.tif',f'_{suffix}.tif')\n",
    "    report_new = report.replace('.pdf',f'_{suffix}.pdf')\n",
    "    \n",
    "    mask_to_gdf(cutline, ortho, ortho_new)\n",
    "    os.rename(report, report_new)\n",
    "\n",
    "    focal_files = [ortho_new, report_new]\n",
    "    \n",
    "    for f in focal_files:\n",
    "        copy_to_gcs(f, output_bucket)\n",
    "    \n",
    "    # Cleanup: Remove temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "def stop_instance(instance_name):\n",
    "    # Construct the gsutil command to stop the instance\n",
    "    cmd = f'gcloud compute instances stop {instance_name}'\n",
    "\n",
    "    try:\n",
    "        # Execute the gsutil command using subprocess\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        print(f'Successfully stopped instance: {instance_name}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'Error stopping instance: {instance_name}')\n",
    "        print(e)\n",
    "\n",
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"\n",
    "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
    "    regions.\n",
    "\n",
    "    Source: https://gist.github.com/pv/8036995\n",
    "    \"\"\"\n",
    "\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()*2\n",
    "\n",
    "    # Construct a map containing all ridges for a given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "\n",
    "            # Compute the missing endpoint of an infinite ridge\n",
    "\n",
    "            t = vor.points[p2] - vor.points[p1] # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "\n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + direction * radius\n",
    "\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "\n",
    "        # sort region counterclockwise\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "\n",
    "        # finish\n",
    "        new_regions.append(new_region.tolist())\n",
    "\n",
    "    return new_regions, np.asarray(new_vertices)\n",
    "\n",
    "def generate_points(poly, num, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    points = []\n",
    "    minx, miny, maxx, maxy = poly.bounds\n",
    "    while len(points) < num:\n",
    "        random_point = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))\n",
    "        if (random_point.within(poly)):\n",
    "            points.append(random_point)\n",
    "    return [point.coords[0] for point in points]\n",
    "\n",
    "def calculate_voronoi_complexity(poly, points):\n",
    "    # compute Voronoi tesselation\n",
    "    vor = Voronoi(points)\n",
    "    \n",
    "    # create finite Voronoi polygons\n",
    "    regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "    \n",
    "    # construct the polygons and intersect with the original one\n",
    "    voronoi_polygons = [poly.intersection(Polygon(vertices[region])) for region in regions]\n",
    "    \n",
    "    # return only the valid ones (completely inside the original polygon)\n",
    "    valid_polygons = [p for p in voronoi_polygons if p.is_valid]\n",
    "\n",
    "    # calculate the perimeter and areas of each valid polygon\n",
    "    perimeters = [p.length for p in valid_polygons]\n",
    "    areas = [p.area for p in valid_polygons]\n",
    "    complexity = [x / y for x, y in zip(perimeters, areas)]\n",
    "    \n",
    "    return complexity, valid_polygons\n",
    "\n",
    "def optimize_voronoi_complexity(poly, num, max_iterations=1000, learning_rate=0.1, seed=None):\n",
    "    points = generate_points(poly, num, seed)\n",
    "    np.random.seed(seed)\n",
    "    mns = [] # to store complexity means at each iteration\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        complexity, polygons = calculate_voronoi_complexity(poly, points)\n",
    "        mn = np.mean(complexity)\n",
    "        mns.append(mn) # append current std dev to the list\n",
    "\n",
    "        # randomly select a point\n",
    "        point_idx = np.random.randint(0, len(points))\n",
    "        current_point = Point(points[point_idx])\n",
    "        \n",
    "        # Compute gradients by trying small movements in each direction\n",
    "        min_mn = mn\n",
    "        min_mn_direction = None\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            new_point = Point(current_point.x + dx * learning_rate, current_point.y + dy * learning_rate)\n",
    "            new_points = points.copy()\n",
    "            new_points[point_idx] = (new_point.x, new_point.y)\n",
    "            \n",
    "            new_complexity, _ = calculate_voronoi_complexity(poly, new_points)\n",
    "            new_mn = np.mean(new_complexity)\n",
    "            \n",
    "            if new_mn < min_mn:\n",
    "                min_mn = new_mn\n",
    "                min_mn_direction = (dx, dy)\n",
    "        \n",
    "        # If we found a direction that decreases the mean complexity, move the point\n",
    "        if min_mn_direction is not None:\n",
    "            dx, dy = min_mn_direction\n",
    "            points[point_idx] = (current_point.x + dx * learning_rate, current_point.y + dy * learning_rate)\n",
    "    \n",
    "    return polygons, mns\n",
    "\n",
    "def log_progress(file, bucket):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write('done')\n",
    "    copy_to_gcs(file, bucket)\n",
    "\n",
    "def filter_gcp_list(file_path, polygon_gdf, output_file_path):\n",
    "    # Check that the polygon_gdf contains a single geometry\n",
    "    if len(polygon_gdf) != 1 or not isinstance(polygon_gdf.geometry.iloc[0], Polygon):\n",
    "        raise ValueError('polygon_gdf must contain a single Polygon geometry')\n",
    "\n",
    "    # Extract the Polygon object\n",
    "    polygon = polygon_gdf.geometry.iloc[0]\n",
    "\n",
    "    # Read header and data\n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline().strip()\n",
    "\n",
    "    # Assuming the header contains the EPSG code in a format like \"EPSG:XXXX\"\n",
    "    epsg_code = header.split(\":\")[-1]\n",
    "\n",
    "    # Read data skipping the header\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', skiprows=1, header=None)\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(data[0], data[1])]\n",
    "    geo_data = gpd.GeoDataFrame(data, geometry=geometry, crs=f'EPSG:{epsg_code}')\n",
    "\n",
    "    # Filter points within the polygon\n",
    "    mask = geo_data.within(polygon)\n",
    "    filtered_geo_data = geo_data[mask]\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    filtered_data = pd.DataFrame(filtered_geo_data)\n",
    "    filtered_data.drop(columns='geometry', inplace=True)\n",
    "\n",
    "    # Write the data to a new file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.write(header + '\\n')\n",
    "\n",
    "    filtered_data.to_csv(output_file_path, sep='\\t', header=False, index=False, mode='a')\n",
    "\n",
    "temp_work = tempfile.mkdtemp()\n",
    "os.chdir(temp_work)\n",
    "\n",
    "branch = 'main'\n",
    "survey = '230612_spurgepoly'\n",
    "array_idx = 3\n",
    "config_url = f'https://raw.githubusercontent.com/samsoe/mpg_aerial_survey/{branch}/surveys/{survey}/config_file.json'\n",
    "\n",
    "#array_idx = int(get_metadata('array_idx')) #dynamic production version\n",
    "#config_url = get_metadata('config_url')#dynamic production version\n",
    "instance_name = f'odm-array-{array_idx}' #name of instance inferred from index\n",
    "\n",
    "config_file = os.path.basename(config_url)\n",
    "download_file(config_url, config_file)\n",
    "\n",
    "with open(config_file, 'r') as json_file:\n",
    "    # Load the JSON data into a Python object\n",
    "    config = json.load(json_file)\n",
    "\n",
    "gcp_res = str(config['gcp_res'])\n",
    "gcp_grid_url = f'https://raw.githubusercontent.com/samsoe/mpg_aerial_survey/{branch}/gcp_kmls/upland_gcps_{gcp_res}m.kml'\n",
    "\n",
    "survey_res = config['survey_res']\n",
    "compute_array_sz = config['compute_array_sz']\n",
    "flight_plan_url = config['flight_plan_url']\n",
    "photo_manifest_url = config['photo_manifest_url']\n",
    "output_bucket = 'mpg-aerial-survey/surveys/230612_spurgepoly/post_process/odm/weird_run_on_node_5/001-double_gcp_30k_min_num'\n",
    "#output_bucket =  config['output_bucket']\n",
    "gcp_editor_url = config['gcp_editor_url']\n",
    "log_bucket = output_bucket + '/logs'\n",
    "\n",
    "log_progress(f'loaded_config_{array_idx}.txt', log_bucket)\n",
    "\n",
    "gcp_grid = os.path.basename(gcp_grid_url)\n",
    "flight_plan = os.path.basename(flight_plan_url)\n",
    "photo_manifest = os.path.basename(photo_manifest_url)\n",
    "\n",
    "download_file(gcp_grid_url, gcp_grid)\n",
    "download_file(flight_plan_url, flight_plan)\n",
    "download_file(photo_manifest_url, photo_manifest)\n",
    "\n",
    "if gcp_editor_url is not None:\n",
    "    gcp_list = os.path.basename(gcp_editor_url)\n",
    "    gcp_list_init = os.path.basename(gcp_list.replace('.txt','_init.txt'))\n",
    "    download_file(gcp_editor_url, gcp_list_init)\n",
    "else:\n",
    "    gcp_list = None\n",
    "\n",
    "log_progress(f'downloaded_supporting_dat_{array_idx}.txt', log_bucket)\n",
    "\n",
    "flight_roi = load_kml(flight_plan)\n",
    "gcps = load_kml(gcp_grid)\n",
    "\n",
    "crs_source = CRS.from_epsg(4326)\n",
    "crs_target = CRS.from_epsg(26911)\n",
    "\n",
    "# Set the source CRS of the GeoDataFrame\n",
    "flight_roi.crs = crs_source\n",
    "gcps.crs = crs_source\n",
    "\n",
    "# Reproject the GeoDataFrame to the target CRS\n",
    "flight_projected_src = flight_roi.to_crs(crs_target)\n",
    "gcps_projected_src = gcps.to_crs(crs_target)\n",
    "\n",
    "gcps_flight = gpd.sjoin(gcps_projected_src, flight_projected_src, how='inner', op='within')\n",
    "\n",
    "parts, means = optimize_voronoi_complexity(flight_projected_src.geometry[0], compute_array_sz, \n",
    "                                         learning_rate=30, max_iterations=1000, seed=0)\n",
    "\n",
    "log_progress(f'partitioned_area_{array_idx}.txt', log_bucket)\n",
    "\n",
    "base_poly = gpd.GeoDataFrame(geometry=[parts[array_idx]], crs = 26911)\n",
    "buffered_poly = expand_to_gcps(base_poly, gcps_flight, step_sz=30)\n",
    "manifest_df = pd.read_csv(photo_manifest)\n",
    "manifest_df['geometry'] = manifest_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "manifest_gpd = gpd.GeoDataFrame(manifest_df, geometry='geometry', crs=crs_source).to_crs(crs_target)\n",
    "target_photos = gpd.sjoin(manifest_gpd, gpd.GeoDataFrame(geometry=buffered_poly), op='within')\n",
    "\n",
    "# if gcp_editor_url is not None:\n",
    "#     filter_gcp_list(gcp_list_init, buffered_poly.to_crs(crs_source), gcp_list)\n",
    "#     os.remove(gcp_list_init)\n",
    "\n",
    "# log_progress(f'started_post_processing_{array_idx}.txt', log_bucket)\n",
    "\n",
    "# process_images(batch=target_photos, output_bucket=output_bucket,\n",
    "#                 ortho_res=survey_res, cutline=base_poly ,suffix=array_idx,\n",
    "#                 gcp_list_path=gcp_list)\n",
    "\n",
    "# log_progress(f'stopping_{array_idx}.txt', log_bucket)\n",
    "#3shutil.rmtree(temp_work)\n",
    "#stop_instance(instance_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_photos.to_csv('/Users/kdoherty/Downloads/test_manifest_230620.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
